{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd6130c2-3a8c-4401-ae9b-c8cc743a074b",
   "metadata": {},
   "source": [
    "# Week 6 Exercise (group): Exploratory Data Analysis on Social Media Data\n",
    "\n",
    "- Caren Chua \n",
    "- Leslie Cohrt\n",
    "- Sarah Auther\n",
    "- Shoshana Medved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603a40a3-9ea2-4d3f-a8c5-2d98336ec4bc",
   "metadata": {},
   "source": [
    "## 1. Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db205416-b52f-46e0-9b5b-5270155cb6ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Using cached textblob-0.17.1-py2.py3-none-any.whl (636 kB)\n",
      "Collecting nltk>=3.1\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk>=3.1->textblob) (1.2.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk>=3.1->textblob) (4.65.0)\n",
      "Collecting regex>=2021.8.3\n",
      "  Using cached regex-2023.5.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (769 kB)\n",
      "Collecting click\n",
      "  Using cached click-8.1.3-py3-none-any.whl (96 kB)\n",
      "Installing collected packages: regex, click, nltk, textblob\n",
      "Successfully installed click-8.1.3 nltk-3.8.1 regex-2023.5.5 textblob-0.17.1\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8047971-3741-4db0-853f-4ae6548443db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting emoji\n",
      "  Using cached emoji-2.2.0-py3-none-any.whl\n",
      "Installing collected packages: emoji\n",
      "Successfully installed emoji-2.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install emoji "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8308e587-737b-44aa-b3ee-de8542e973f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d4f36b4-0c75-49f5-97b0-148cf20eba44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db233911-c1c9-4ef4-a616-68c46f0f7593",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3eb44a6-2c62-4e8d-a822-febfddf86c4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81ca74b1-f6a4-4220-991f-2f404c970e0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d4a085-74e8-4093-85e4-9a2f8008bf04",
   "metadata": {},
   "source": [
    "## 2. Read the data\n",
    "\n",
    "The data is called `tweets.csv` in the same folder. More information about the data see [here](https://www.kaggle.com/datasets/infamouscoder/mental-health-social-media)\n",
    "\n",
    "The main column you will be working with is `post_text`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c10a6b2a-52c4-4409-ba19-f199bff034b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>post_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>followers</th>\n",
       "      <th>friends</th>\n",
       "      <th>favourites</th>\n",
       "      <th>statuses</th>\n",
       "      <th>retweets</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20000.000000</td>\n",
       "      <td>2.000000e+04</td>\n",
       "      <td>2.000000e+04</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>2.000000e+04</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>9999.500000</td>\n",
       "      <td>6.874728e+17</td>\n",
       "      <td>3.548623e+16</td>\n",
       "      <td>900.483950</td>\n",
       "      <td>782.428750</td>\n",
       "      <td>6398.235550</td>\n",
       "      <td>4.439442e+04</td>\n",
       "      <td>1437.927300</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5773.647028</td>\n",
       "      <td>1.708396e+17</td>\n",
       "      <td>1.606083e+17</td>\n",
       "      <td>1899.913961</td>\n",
       "      <td>1834.817945</td>\n",
       "      <td>8393.072914</td>\n",
       "      <td>1.407785e+05</td>\n",
       "      <td>15119.665118</td>\n",
       "      <td>0.500013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.555966e+09</td>\n",
       "      <td>1.472438e+07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4999.750000</td>\n",
       "      <td>5.931686e+17</td>\n",
       "      <td>3.242944e+08</td>\n",
       "      <td>177.000000</td>\n",
       "      <td>211.000000</td>\n",
       "      <td>243.000000</td>\n",
       "      <td>5.129000e+03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9999.500000</td>\n",
       "      <td>7.637400e+17</td>\n",
       "      <td>1.052122e+09</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>561.000000</td>\n",
       "      <td>2752.000000</td>\n",
       "      <td>1.325100e+04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>14999.250000</td>\n",
       "      <td>8.153124e+17</td>\n",
       "      <td>2.285923e+09</td>\n",
       "      <td>1197.000000</td>\n",
       "      <td>701.000000</td>\n",
       "      <td>8229.000000</td>\n",
       "      <td>5.289200e+04</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>19999.000000</td>\n",
       "      <td>8.194574e+17</td>\n",
       "      <td>7.631825e+17</td>\n",
       "      <td>28614.000000</td>\n",
       "      <td>28514.000000</td>\n",
       "      <td>39008.000000</td>\n",
       "      <td>1.063601e+06</td>\n",
       "      <td>839540.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0       post_id       user_id     followers       friends  \\\n",
       "count  20000.000000  2.000000e+04  2.000000e+04  20000.000000  20000.000000   \n",
       "mean    9999.500000  6.874728e+17  3.548623e+16    900.483950    782.428750   \n",
       "std     5773.647028  1.708396e+17  1.606083e+17   1899.913961   1834.817945   \n",
       "min        0.000000  3.555966e+09  1.472438e+07      0.000000      0.000000   \n",
       "25%     4999.750000  5.931686e+17  3.242944e+08    177.000000    211.000000   \n",
       "50%     9999.500000  7.637400e+17  1.052122e+09    476.000000    561.000000   \n",
       "75%    14999.250000  8.153124e+17  2.285923e+09   1197.000000    701.000000   \n",
       "max    19999.000000  8.194574e+17  7.631825e+17  28614.000000  28514.000000   \n",
       "\n",
       "         favourites      statuses       retweets         label  \n",
       "count  20000.000000  2.000000e+04   20000.000000  20000.000000  \n",
       "mean    6398.235550  4.439442e+04    1437.927300      0.500000  \n",
       "std     8393.072914  1.407785e+05   15119.665118      0.500013  \n",
       "min        0.000000  3.000000e+00       0.000000      0.000000  \n",
       "25%      243.000000  5.129000e+03       0.000000      0.000000  \n",
       "50%     2752.000000  1.325100e+04       0.000000      0.500000  \n",
       "75%     8229.000000  5.289200e+04       1.000000      1.000000  \n",
       "max    39008.000000  1.063601e+06  839540.000000      1.000000  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = \n",
    "\n",
    "df = pd.read_csv(\"tweets.csv\")\n",
    "\n",
    "# explore the data characteristic using `df.describe()` or `df.info()`\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13566cfc-ca98-4a22-89a0-c0774944d289",
   "metadata": {},
   "source": [
    "## 3. Extract emojis\n",
    "\n",
    "Use `emoji` package to extract emojis and put them into a new column called `emojis`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2d03e40d-0310-4654-9100-26f4442c17d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define the function\n",
    "def extract_emojis(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.findall(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a1b011cf-af77-4848-b4de-a019af042068",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        []\n",
       "1        []\n",
       "2        []\n",
       "3        []\n",
       "4        []\n",
       "         ..\n",
       "19995    []\n",
       "19996    []\n",
       "19997    []\n",
       "19998    []\n",
       "19999    []\n",
       "Name: emojis, Length: 20000, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply the function to your dataframe\n",
    "df['emojis'] = df['post_text'].apply(extract_emojis)\n",
    "df['emojis']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d961f7-b745-4f4d-b516-68d9ef48dce2",
   "metadata": {},
   "source": [
    "## 4. Text Cleaning using Regular Expressions \n",
    "\n",
    "1. Remove URLs\n",
    "2. Remove mentions\n",
    "3. Remove hashtags\n",
    "4. Remove special characters\n",
    "5. Remove extra space\n",
    "\n",
    "Code can be found in [week 6 lecture 1](https://github.com/yibeichan/COMM160DS/blob/main/week_6/lecture_part1.ipynb) section `4.4 All-in-One`\n",
    "\n",
    "Perform the analysis and save the results into a new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b2f10ffa-9ce2-4e3a-b300-304e44733ca1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        It's just over 2 years since I was diagnosed w...\n",
       "1        It's Sunday, I need a break, so I'm planning t...\n",
       "2        Awake but tired. I need to sleep but my brain ...\n",
       "3        RT @SewHQ: #Retro bears make perfect gifts and...\n",
       "4        It’s hard to say whether packing lists are mak...\n",
       "                               ...                        \n",
       "19995                A day without sunshine is like night.\n",
       "19996    Boren's Laws: (1) When in charge, ponder. (2) ...\n",
       "19997    The flow chart is a most thoroughly oversold p...\n",
       "19998    Ships are safe in harbor, but they were never ...\n",
       "19999       Black holes are where God is dividing by zero.\n",
       "Name: post_text, Length: 20000, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"post_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ec34e8bc-47ab-4bc9-a623-9436ecf6014b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        It's just over 2 years since I was diagnosed w...\n",
      "1        It's Sunday, I need a break, so I'm planning t...\n",
      "2        Awake but tired. I need to sleep but my brain ...\n",
      "3        RT @SewHQ: #Retro bears make perfect gifts and...\n",
      "4        It’s hard to say whether packing lists are mak...\n",
      "                               ...                        \n",
      "19995                A day without sunshine is like night.\n",
      "19996    Boren's Laws: (1) When in charge, ponder. (2) ...\n",
      "19997    The flow chart is a most thoroughly oversold p...\n",
      "19998    Ships are safe in harbor, but they were never ...\n",
      "19999       Black holes are where God is dividing by zero.\n",
      "Name: post_text, Length: 20000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)  # remove URLs\n",
    "    text = re.sub(r'@\\w+', '', text)  # remove mentions\n",
    "    text = re.sub(r'@\\w+', '', text)  # remove hashtag\n",
    "    text = re.sub(r'\\W', ' ', text)  # remove special characters\n",
    "    text = re.sub(r'\\s+', ' ', text)  # remove extra spaces\n",
    "    return text.strip()\n",
    "print(text)\n",
    "\n",
    "# apply the function to your dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec44d9f5-2311-4379-ba10-bd9a4ba87c8e",
   "metadata": {},
   "source": [
    "## 5. Analysis 1 (Sentiment Analysis)\n",
    "\n",
    "Choose one analysis from (1)Sentiment Analysis, (2)N-grams and Phrase Analysis, (3)Collocation Analysis, (4)Part-of-Speech Tagging, (5)Named Entity Recognition, and (6)Dependency Parsing.\n",
    "\n",
    "Perform the analysis and save the results into a new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3bb995d2-845a-429e-ad81-b49911914de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here\n",
    "\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "def get_sentiment(text):\n",
    "    analysis = TextBlob(text)\n",
    "    if analysis.sentiment.polarity > 0:\n",
    "        return 'positive'\n",
    "    elif analysis.sentiment.polarity == 0:\n",
    "        return 'neutral'\n",
    "    else:\n",
    "        return 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f7ceada1-2b0e-435d-a2d7-88d9a303b05b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...\n",
       "1        {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...\n",
       "2        {'neg': 0.259, 'neu': 0.741, 'pos': 0.0, 'comp...\n",
       "3        {'neg': 0.0, 'neu': 0.715, 'pos': 0.285, 'comp...\n",
       "4        {'neg': 0.06, 'neu': 0.819, 'pos': 0.121, 'com...\n",
       "                               ...                        \n",
       "19995    {'neg': 0.542, 'neu': 0.458, 'pos': 0.0, 'comp...\n",
       "19996    {'neg': 0.257, 'neu': 0.743, 'pos': 0.0, 'comp...\n",
       "19997    {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...\n",
       "19998    {'neg': 0.0, 'neu': 0.86, 'pos': 0.14, 'compou...\n",
       "19999    {'neg': 0.0, 'neu': 0.792, 'pos': 0.208, 'comp...\n",
       "Name: sent_an, Length: 20000, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "df['sent_an'] = df['post_text'].apply(lambda x: sia.polarity_scores(x))\n",
    "df['sent_an']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff6ee3d-825a-4ef0-9193-8d69e95b29ad",
   "metadata": {},
   "source": [
    "## 6. Analysis 2 (N-grams)\n",
    "\n",
    "Choose another analysis from (1)Sentiment Analysis, (2)N-grams and Phrase Analysis, (3)Collocation Analysis, (4)Part-of-Speech Tagging, (5)Named Entity Recognition, and (6)Dependency Parsing.\n",
    "\n",
    "Perform the analysis and save the results into a new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f15c89b2-634b-4526-a67f-26806ca3aeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here\n",
    "from nltk import ngrams\n",
    "\n",
    "def generate_ngrams(text, n):\n",
    "    tokens = text.split()\n",
    "    return list(ngrams(tokens, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d8f72b83-1a54-42a8-a5f2-24f96187fafb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [(It's, just), (just, over), (over, 2), (2, ye...\n",
       "1        [(It's, Sunday,), (Sunday,, I), (I, need), (ne...\n",
       "2        [(Awake, but), (but, tired.), (tired., I), (I,...\n",
       "3        [(RT, @SewHQ:), (@SewHQ:, #Retro), (#Retro, be...\n",
       "4        [(It’s, hard), (hard, to), (to, say), (say, wh...\n",
       "                               ...                        \n",
       "19995    [(A, day), (day, without), (without, sunshine)...\n",
       "19996    [(Boren's, Laws:), (Laws:, (1)), ((1), When), ...\n",
       "19997    [(The, flow), (flow, chart), (chart, is), (is,...\n",
       "19998    [(Ships, are), (are, safe), (safe, in), (in, h...\n",
       "19999    [(Black, holes), (holes, are), (are, where), (...\n",
       "Name: ngram, Length: 20000, dtype: object"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['ngram'] = df['post_text'].apply(lambda x: generate_ngrams(x, n=2))\n",
    "df['ngram']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2adeba-0c61-4216-9042-e79337da575c",
   "metadata": {},
   "source": [
    "## 7. Push Your Results to GitHub\n",
    "\n",
    "As you did in previous weeks:\n",
    "1. `git status`\n",
    "2. `git add`\n",
    "3. `git commit -m \"type your message here\"`\n",
    "4. `git push`\n",
    "\n",
    "If you can't push it to GitHub, it's okay to manually uploaded it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
